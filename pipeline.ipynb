{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC 522 Group 8 Project\n",
    "\n",
    "## Professor Insights: Clustering Quality and Difficulty Across NCSU Colleges\n",
    "\n",
    "This project is a data science project that uses the [RateMyProfessors](https://www.ratemyprofessors.com/) website to scrape data on professors and their ratings. The data is then used to cluster the professors based on their quality and difficulty ratings, and identify similarities and differences of the rating distributions across colleges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Aquisition\n",
    "This cell scrapes the most up-to-date data on all NCSU professors on RateMyProfessors and saves the data to a JSON file. This takes less than 30 seconds to execute, and does not need to be re-run if the file already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ratemyprofessors import RateMyProfessorsAPI\n",
    "\n",
    "\n",
    "api = RateMyProfessorsAPI()\n",
    "school = api.search_school(\"NCSU\")\n",
    "\n",
    "professors = []\n",
    "\n",
    "# Get all NCSU professors on RateMyProfessors\n",
    "cursor = \"\"\n",
    "while True:\n",
    "    result = api.search_teachers(school['id'], \"\", limit=1000, cursor=cursor)\n",
    "    if not result['teachers']:\n",
    "        break\n",
    "    cursor = result['end_cursor']\n",
    "    for professor in result['teachers']:\n",
    "        del professor['school']\n",
    "    professors.extend(result['teachers'])\n",
    "\n",
    "print(len(professors), 'professors fetched')\n",
    "\n",
    "# Filter out professors with no ratings\n",
    "professors = list(filter(lambda professor: professor['num_ratings'] > 0, professors))\n",
    "\n",
    "print(len(professors), 'professors with ratings')\n",
    "\n",
    "# Sort by most ratings in descending order    \n",
    "professors.sort(key=lambda professor: professor['num_ratings'], reverse=True)\n",
    "\n",
    "with open(\"data/professors.json\", \"w\") as file:\n",
    "    json.dump(professors, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell scrapes distributions for every course from the gradient database, calculating the average GPA for each section. It is designed to be able to be computed in multiple executions in case the authorization tokens expire before the scraping is complete. This process takes multiple hours to complete.\n",
    "\n",
    "To execute this cell, you must have valid authentication headers in a file named `gradient-headers.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gradient import GradientAPI\n",
    "\n",
    "\n",
    "with open(\"data/colleges.json\") as file:\n",
    "    colleges = json.load(file)\n",
    "\n",
    "try:\n",
    "    with open(\"data/distributions.json\", \"r\") as file:\n",
    "        distributions = json.load(file)\n",
    "        last_subject, last_course = distributions[-1][\"courseName\"].split()[:2]\n",
    "except:\n",
    "    distributions = []\n",
    "    last_subject = last_course = None\n",
    "\n",
    "\n",
    "api = GradientAPI(request_delay=2)\n",
    "\n",
    "found = False\n",
    "try:\n",
    "    for college in colleges:\n",
    "        for subject in colleges[college][\"subjects\"]:\n",
    "            # Check if we have already fetched this subject\n",
    "            if last_subject and not found:\n",
    "                if subject != last_subject:\n",
    "                    continue\n",
    "                found = True\n",
    "            \n",
    "            for course_distributions in api.get_subject_distrubutions(subject, last_course if subject == last_subject else None):\n",
    "                if \"individual\" not in course_distributions:\n",
    "                    continue\n",
    "                \n",
    "                for section in course_distributions[\"individual\"]:\n",
    "                    a, b, c, d, f, s, u, w = (section[\"grades\"][grade][\"raw\"] for grade in [\"A\", \"B\", \"C\", \"D\", \"F\", \"S\", \"U\", \"W\"])\n",
    "                    total = a + b + c + d + f + s + u + w\n",
    "                    gpa = round((a * 4 + b * 3 + c * 2 + d + s * 3) / total, 2) if total else 0\n",
    "                    \n",
    "                    section[\"gpa\"] = gpa\n",
    "                    section[\"total\"] = total\n",
    "                    section[\"college\"] = college\n",
    "                    del section[\"grades\"]\n",
    "                    del section[\"googleChart\"]\n",
    "                \n",
    "                distributions.extend(course_distributions[\"individual\"])\n",
    "except:\n",
    "    # gracefully handle exceptions or interruptions\n",
    "    pass\n",
    "finally:\n",
    "    with open(\"data/distributions.json\", \"w\") as file:\n",
    "        json.dump(distributions, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing\n",
    "\n",
    "This cell processes the data and tags each professor with their college using a file generated by an LLM and manually validated that maps college names to department names. It then combines duplicate professors (same name and same department) into a single entry with a weighted average of their ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "\n",
    "with open(\"data/professors.json\", \"r\") as file:\n",
    "    professors = json.load(file)\n",
    "    \n",
    "with open(\"data/colleges.json\", \"r\") as file:\n",
    "    colleges = json.load(file)\n",
    "\n",
    "\n",
    "# Construct a map from department to college\n",
    "department_map = {department: college for college, departments in colleges.items() for department in departments}\n",
    "\n",
    "# Fuzzy search for the best department match and assign the college\n",
    "for professor in professors:\n",
    "    department = professor['department']\n",
    "    result = difflib.get_close_matches(department, department_map.keys(), cutoff=0.75)\n",
    "    professor['college'] = department_map[result[0]] if result else None\n",
    "\n",
    "\n",
    "# Combine duplicate entries of the same professor that belong to the same college\n",
    "names = set()\n",
    "duplicates_names = []\n",
    "\n",
    "for professor in professors:\n",
    "    name = professor['name']\n",
    "    if name in names and name not in duplicates_names:\n",
    "        duplicates_names.append(name)\n",
    "    names.add(name)\n",
    "\n",
    "duplicates = {}\n",
    "for professor in filter(lambda professor: professor['name'] in duplicates_names, professors):\n",
    "    key = (professor['name'], professor['college'])\n",
    "    if key not in duplicates:\n",
    "        duplicates[key] = professor\n",
    "    else:\n",
    "        # Take weighted average of two entries\n",
    "        n1, n2 = duplicates[key]['num_ratings'], professor['num_ratings']\n",
    "        total = n1 + n2\n",
    "        avg1, avg2 = duplicates[key]['avg_rating'], professor['avg_rating']\n",
    "        take1, take2 = duplicates[key]['would_take_again'], professor['would_take_again']\n",
    "        diff1, diff2 = duplicates[key]['avg_difficulty'], professor['avg_difficulty']\n",
    "        \n",
    "        duplicates[key]['num_ratings'] = total\n",
    "        duplicates[key]['avg_rating'] = round((avg1 * n1 + avg2 * n2) / total, 1)\n",
    "        duplicates[key]['would_take_again'] = round((take1 * n1 + take2 * n2) / total, 1)\n",
    "        duplicates[key]['avg_difficulty'] = round((diff1 * n1 + diff2 * n2) / total, 1)\n",
    "    \n",
    "\n",
    "professors = list(filter(lambda professor: professor['name'] not in duplicates_names, professors)) + list(duplicates.values())\n",
    "\n",
    "\n",
    "with open(\"data/professors.json\", \"w\") as file:\n",
    "    json.dump(professors, file, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell processes and aggregates the gradient data for all professors that exist on ratemyprofessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"data/professors.json\", \"r\") as file:\n",
    "    professors = {professor[\"name\"].lower(): professor for professor in json.load(file)}  # {name: {professor info}}\n",
    "    professor_names = [professor[\"name\"].lower() for professor in professors.values()]\n",
    "    \n",
    "with open(\"data/distributions.json\", \"r\") as file:\n",
    "    distributions = json.load(file)\n",
    "\n",
    "# To be filled in manually and then converted to dataframe (and then stored as .csv)\n",
    "professors_info = {\"Name\": [], \"College\": [], \"Quality Score\": [], \"Difficulty Score\": [], \"GPA\": [], \"Would Take Again\": [], \"Number of Ratings\": [], \"Number of Sections\": [], \"Total Students\": []}\n",
    "\n",
    "name_map = {}\n",
    "not_found = []\n",
    "missed_names = set()\n",
    "collisions = 0\n",
    "for section in distributions:\n",
    "    full_name = \" \".join(section[\"instructorName\"].split(\",\", 1)[::-1])\n",
    "\n",
    "    # drop middle name and roman numeral and phd when searching for name\n",
    "    split_name = full_name.split()\n",
    "    if all(c in 'IV' for c in split_name[-1].upper()) or split_name[-1].lower() == 'phd':\n",
    "        name = ' '.join([split_name[0], split_name[-2]])\n",
    "    else:\n",
    "        name = ' '.join([split_name[0], split_name[-1]])\n",
    "    result = difflib.get_close_matches(name.lower(), professor_names, n=1, cutoff=0.89)\n",
    "    if not result:\n",
    "        not_found.append(section)\n",
    "        missed_names.add(full_name)\n",
    "        continue\n",
    "    result = result[0]\n",
    "\n",
    "    # fill in professor's info in df\n",
    "    prof_info = professors[result]\n",
    "    try:\n",
    "        # case: professor already added to table, just accumulate stats\n",
    "        idx = professors_info[\"Name\"].index(prof_info['name'])\n",
    "        professors_info[\"GPA\"][idx] += section['gpa'] * section['total']\n",
    "        professors_info[\"Number of Sections\"][idx] += 1\n",
    "        professors_info[\"Total Students\"][idx] += section['total']\n",
    "    except:\n",
    "        # case: professor not in table yet, initialize everything\n",
    "        professors_info[\"Name\"].append(prof_info['name'])\n",
    "        professors_info[\"College\"].append(prof_info['college'])\n",
    "        professors_info[\"Quality Score\"].append(prof_info['avg_rating'])\n",
    "        professors_info[\"Difficulty Score\"].append(prof_info['avg_difficulty'])\n",
    "        professors_info[\"GPA\"].append(section['gpa'] * section['total'])\n",
    "        professors_info[\"Would Take Again\"].append(prof_info['would_take_again'])\n",
    "        professors_info[\"Number of Ratings\"].append(prof_info['num_ratings'])\n",
    "        professors_info[\"Number of Sections\"].append(1)\n",
    "        professors_info[\"Total Students\"].append(section['total'])\n",
    "\n",
    "# Average out GPA\n",
    "for i in range(len(professors_info[\"GPA\"])):\n",
    "    professors_info[\"GPA\"][i] /= professors_info[\"Total Students\"][i]\n",
    "\n",
    "# convert to df and store as csv and json\n",
    "df = pd.DataFrame(professors_info)\n",
    "df.to_json(\"data/combined_data.json\", compression='infer')\n",
    "df.to_csv(\"data/combined_data.csv\", compression='infer')\n",
    "\n",
    "# save items not found\n",
    "with open(\"data/sections_not_found.json\", \"w\") as file:\n",
    "    json.dump(not_found, file, indent=2)\n",
    "\n",
    "# save unique names that weren't found\n",
    "with open(\"data/missed_names.txt\", \"w\") as file:\n",
    "    file.write(str(missed_names))\n",
    "\n",
    "#     if result in name_map.keys() and full_name not in name_map[result]:\n",
    "#         name_map[result].append(full_name)\n",
    "#         collisions += 1\n",
    "#     else:\n",
    "#         name_map[result] = [full_name]\n",
    "\n",
    "# print(f'Collisions: {collisions}\\n')\n",
    "# print(f'Not found: {len(not_found)} items')\n",
    "# print(not_found)\n",
    "# print()\n",
    "# print([(key, value) for (key, value) in name_map.items() if len(value) > 1])\n",
    "\n",
    "\n",
    "    # TODO: join the ratemyprofessors and gradient data into a single json/csv, drop professors that don't exist in both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number missed: 4317\n"
     ]
    }
   ],
   "source": [
    "# DISREGARD\n",
    "# special_cases = [\"Nick Alexander Longerbeam\", \"Joseph A Johnson\", \"Paul I Ro\", \"Zo Webster\"]\n",
    "# for i in special_cases:\n",
    "#     not_found.discard(i)\n",
    "# relevant_sections = [sect for sect in distributions if \" \".join(sect[\"instructorName\"].split(\",\", 1)[::-1]) not in not_found]\n",
    "# print(f'Number of found items: {len(relevant_sections)}')\n",
    "\n",
    "# with open(\"further_filtered_distributions.json\", \"w\") as file:\n",
    "#     json.dump(relevant_sections, file, indent=2)\n",
    "print(f'Number missed: {len(missed_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Processing\n",
    "This stage involves the application of a tuned clusternig algorithm to the data. The clustering algorithm will group similar data points together, and evaulate the homogeniety of the classes of the clusters (the similiarity of professors in the same departments). This step allows us to identify patterns and relationships within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Decide on clustering aglorithm and tuning approach\n",
    "# TODO: Perform tuned clustering on professor data points\n",
    "# TODO: Calculate homogeniety of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualization\n",
    "This stage involves generating visuals to analyze and find patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize data points and rating distributions for each college individually\n",
    "# TODO: Visualize aggregation + average of data points and rating distributions for all colleges\n",
    "# TODO: Visualize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Interpretation\n",
    "This stage involves computing metrics to drive insights about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate and compare weighted averages of ratings for each college, compare to global averages\n",
    "# TODO: Calculate metrics like standard deviation, median, outliers, for each college and globally\n",
    "# TODO: Compare expected salary with average ratings for each college"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
