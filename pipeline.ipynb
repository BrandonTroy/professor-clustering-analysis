{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC 522 Group 8 Project\n",
    "\n",
    "## Professor Insights: Clustering Quality and Difficulty Across NCSU Colleges\n",
    "\n",
    "This project is a data science project that uses the [RateMyProfessors](https://www.ratemyprofessors.com/) website to scrape data on professors and their ratings. The data is then used to cluster the professors based on their quality and difficulty ratings, and identify similarities and differences of the rating distributions across colleges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Aquisition\n",
    "This cell scrapes the most up-to-date data on all NCSU professors on RateMyProfessors and saves the data to a JSON file. This takes less than 30 seconds to execute, and does not need to be re-run if the file already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ratemyprofessors import RateMyProfessorsAPI\n",
    "\n",
    "\n",
    "api = RateMyProfessorsAPI()\n",
    "school = api.search_school(\"NCSU\")\n",
    "\n",
    "professors = []\n",
    "\n",
    "# Get all NCSU professors on RateMyProfessors\n",
    "cursor = \"\"\n",
    "while True:\n",
    "    result = api.search_teachers(school['id'], \"\", limit=1000, cursor=cursor)\n",
    "    if not result['teachers']:\n",
    "        break\n",
    "    cursor = result['end_cursor']\n",
    "    for professor in result['teachers']:\n",
    "        del professor['school']\n",
    "    professors.extend(result['teachers'])\n",
    "\n",
    "print(len(professors), 'professors fetched')\n",
    "\n",
    "# Filter out professors with no ratings\n",
    "professors = list(filter(lambda professor: professor['num_ratings'] > 0, professors))\n",
    "\n",
    "print(len(professors), 'professors with ratings')\n",
    "\n",
    "# Sort by most ratings in descending order    \n",
    "professors.sort(key=lambda professor: professor['num_ratings'], reverse=True)\n",
    "\n",
    "with open(\"data/professors.json\", \"w\") as file:\n",
    "    json.dump(professors, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell scrapes distributions for every course from the gradient database, calculating the average GPA for each section. It is designed to be able to be computed in multiple executions in case the authorization tokens expire before the scraping is complete. This process takes multiple hours to complete.\n",
    "\n",
    "To execute this cell, you must have valid authentication headers in a file named `gradient-headers.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gradient import GradientAPI\n",
    "\n",
    "\n",
    "with open(\"data/colleges.json\") as file:\n",
    "    colleges = json.load(file)\n",
    "\n",
    "try:\n",
    "    with open(\"data/distributions.json\", \"r\") as file:\n",
    "        distributions = json.load(file)\n",
    "        last_subject, last_course = distributions[-1][\"courseName\"].split()[:2]\n",
    "except:\n",
    "    distributions = []\n",
    "    last_subject = last_course = None\n",
    "\n",
    "\n",
    "api = GradientAPI(request_delay=2)\n",
    "\n",
    "found = False\n",
    "try:\n",
    "    for college in colleges:\n",
    "        for subject in colleges[college][\"subjects\"]:\n",
    "            # Check if we have already fetched this subject\n",
    "            if last_subject and not found:\n",
    "                if subject != last_subject:\n",
    "                    continue\n",
    "                found = True\n",
    "            \n",
    "            for course_distributions in api.get_subject_distrubutions(subject, last_course if subject == last_subject else None):\n",
    "                if \"individual\" not in course_distributions:\n",
    "                    continue\n",
    "                \n",
    "                for section in course_distributions[\"individual\"]:\n",
    "                    a, b, c, d, f, s, u, w = (section[\"grades\"][grade][\"raw\"] for grade in [\"A\", \"B\", \"C\", \"D\", \"F\", \"S\", \"U\", \"W\"])\n",
    "                    total = a + b + c + d + f + s + u + w\n",
    "                    gpa = round((a * 4 + b * 3 + c * 2 + d + s * 3) / total, 2) if total else 0\n",
    "                    \n",
    "                    section[\"gpa\"] = gpa\n",
    "                    section[\"total\"] = total\n",
    "                    section[\"college\"] = college\n",
    "                    del section[\"grades\"]\n",
    "                    del section[\"googleChart\"]\n",
    "                \n",
    "                distributions.extend(course_distributions[\"individual\"])\n",
    "except:\n",
    "    # gracefully handle exceptions or interruptions\n",
    "    pass\n",
    "finally:\n",
    "    with open(\"data/distributions.json\", \"w\") as file:\n",
    "        json.dump(distributions, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing\n",
    "\n",
    "This cell processes the data and tags each professor with their college using a file generated by an LLM and manually validated that maps college names to department names. It then combines duplicate professors (same name and same department) into a single entry with a weighted average of their ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "\n",
    "with open(\"data/professors.json\", \"r\") as file:\n",
    "    professors = json.load(file)\n",
    "    \n",
    "with open(\"data/colleges.json\", \"r\") as file:\n",
    "    colleges = json.load(file)\n",
    "\n",
    "\n",
    "# Construct a map from department to college\n",
    "department_map = {department: college for college, departments in colleges.items() for department in departments}\n",
    "\n",
    "# Fuzzy search for the best department match and assign the college\n",
    "for professor in professors:\n",
    "    department = professor['department']\n",
    "    result = difflib.get_close_matches(department, department_map.keys(), cutoff=0.75)\n",
    "    professor['college'] = department_map[result[0]] if result else None\n",
    "\n",
    "\n",
    "# Combine duplicate entries of the same professor that belong to the same college\n",
    "names = set()\n",
    "duplicates_names = []\n",
    "\n",
    "for professor in professors:\n",
    "    name = professor['name']\n",
    "    if name in names and name not in duplicates_names:\n",
    "        duplicates_names.append(name)\n",
    "    names.add(name)\n",
    "\n",
    "duplicates = {}\n",
    "for professor in filter(lambda professor: professor['name'] in duplicates_names, professors):\n",
    "    key = (professor['name'], professor['college'])\n",
    "    if key not in duplicates:\n",
    "        duplicates[key] = professor\n",
    "    else:\n",
    "        # Take weighted average of two entries\n",
    "        n1, n2 = duplicates[key]['num_ratings'], professor['num_ratings']\n",
    "        total = n1 + n2\n",
    "        avg1, avg2 = duplicates[key]['avg_rating'], professor['avg_rating']\n",
    "        take1, take2 = duplicates[key]['would_take_again'], professor['would_take_again']\n",
    "        diff1, diff2 = duplicates[key]['avg_difficulty'], professor['avg_difficulty']\n",
    "        \n",
    "        duplicates[key]['num_ratings'] = total\n",
    "        duplicates[key]['avg_rating'] = round((avg1 * n1 + avg2 * n2) / total, 1)\n",
    "        duplicates[key]['would_take_again'] = round((take1 * n1 + take2 * n2) / total, 1)\n",
    "        duplicates[key]['avg_difficulty'] = round((diff1 * n1 + diff2 * n2) / total, 1)\n",
    "    \n",
    "\n",
    "professors = list(filter(lambda professor: professor['name'] not in duplicates_names, professors)) + list(duplicates.values())\n",
    "\n",
    "\n",
    "with open(\"data/professors.json\", \"w\") as file:\n",
    "    json.dump(professors, file, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell processes and aggregates the gradient data for all professors that exist on ratemyprofessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "with open(\"data/professors.json\", \"r\") as file:\n",
    "    professors = json.load(file)\n",
    "    professors = {professor[\"id\"]: professor for professor in professors}\n",
    "    ids = {(professor[\"name\"], professor[\"college\"]): professor[\"id\"] for professor in professors.values()}\n",
    "    professor_names = [professor[\"name\"] for professor in professors.values()]\n",
    "\n",
    "with open(\"data/colleges.json\", \"r\") as file:  \n",
    "    colleges = json.load(file)\n",
    "    \n",
    "with open(\"data/istributions.json\", \"r\") as file:\n",
    "    distributions = json.load(file)\n",
    "\n",
    "professor_distributions = {}\n",
    "\n",
    "for section in distributions:\n",
    "    name = \" \".join(section[\"instructorName\"].split(\",\", 1)[::-1])\n",
    "    result = difflib.get_close_matches(name, professor_names, n=1 cutoff=0.75)\n",
    "    if not result:\n",
    "        continue\n",
    "    \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Processing\n",
    "This stage involves the application of a tuned clusternig algorithm to the data. The clustering algorithm will group similar data points together, and evaulate the homogeniety of the classes of the clusters (the similiarity of professors in the same departments). This step allows us to identify patterns and relationships within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Decide on clustering aglorithm and tuning approach\n",
    "# TODO: Perform tuned clustering on professor data points\n",
    "# TODO: Calculate homogeniety of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualization\n",
    "This stage involves generating visuals to analyze and find patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize data points and rating distributions for each college individually\n",
    "# TODO: Visualize aggregation + average of data points and rating distributions for all colleges\n",
    "# TODO: Visualize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Interpretation\n",
    "This stage involves computing metrics to drive insights about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate and compare weighted averages of ratings for each college, compare to global averages\n",
    "# TODO: Calculate metrics like standard deviation, median, outliers, for each college and globally\n",
    "# TODO: Compare expected salary with average ratings for each college"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
